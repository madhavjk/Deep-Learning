{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_encoding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhavjk/Deep-Learning/blob/master/BERT_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI0HtrcPTTFe"
      },
      "source": [
        "# Encoding text with BERT\n",
        "\n",
        "In this notebook we will:\n",
        "\n",
        "1.   Feed text into BERT to get encoded representations\n",
        "2.   Explore how the output word embeddings are context-dependent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_rhrRX5n5gX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "4e043c4c-82bd-4b60-b344-9e091df1d4b0"
      },
      "source": [
        "# We'll use Huggingface's Transformers package\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.38)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.38)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AVjtw5rp4QF"
      },
      "source": [
        "\n",
        "**Preparing the input**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGZsXoF5nbdQ"
      },
      "source": [
        "# This is the name of a pre-trained BERT model available online and downloadable using the transformers package\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "# Use AutoModel to figure out which Transformer to use based on the name of the pretrained model\n",
        "# Note this downloading will be substantial if a bunch of people are doing it. Is there a way we can cache it in a shared Colab environment somehow?\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Get the tokenizer that will help in preprocessing\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3fvgZxxqDEy"
      },
      "source": [
        "The tokenizer breaks words down into smaller wordpieces. In the case of BERT, the wordpieces are prefixed with ## if they are not a whole word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ICKA4iepm1M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "10bc0baa-8b25-4235-8e4b-bf31e21052d4"
      },
      "source": [
        "sentence = \"Unaccountable words are handled with byte pair encoding.\"\n",
        "tokenizer.tokenize(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['una',\n",
              " '##cco',\n",
              " '##unt',\n",
              " '##able',\n",
              " 'words',\n",
              " 'are',\n",
              " 'handled',\n",
              " 'with',\n",
              " 'byte',\n",
              " 'pair',\n",
              " 'encoding',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2kLWIvWqjo9"
      },
      "source": [
        "To fully prepare this sentence for input into the model:\n",
        "\n",
        "*   Special [CLS] and [SEP] tokens must be added to the start of the end, \n",
        "*   The tokens must be converted to corresponding numerical IDs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmLNye3ErAK0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00b2b0d3-dc66-42d6-bcbf-2e0578ebf88e"
      },
      "source": [
        "# Combining wordpieces with special tokens\n",
        "pieces = tokenizer.tokenize(sentence)\n",
        "pieces = [tokenizer.cls_token] + pieces + [tokenizer.sep_token]\n",
        "print(pieces)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'una', '##cco', '##unt', '##able', 'words', 'are', 'handled', 'with', 'byte', 'pair', 'encoding', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eob-B_U-r0c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "c848ce74-982c-47c5-c179-6477f8f58816"
      },
      "source": [
        "# Converting all tokens to numerical IDs\n",
        "tokenizer.convert_tokens_to_ids(pieces)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 14477,\n",
              " 21408,\n",
              " 16671,\n",
              " 3085,\n",
              " 2616,\n",
              " 2024,\n",
              " 8971,\n",
              " 2007,\n",
              " 24880,\n",
              " 3940,\n",
              " 17181,\n",
              " 1012,\n",
              " 102]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQ1xSw3sF50"
      },
      "source": [
        "In most cases it's best to just roll these in to one step with tokenizer.encode()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS637T8FsKo6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "0cc3487e-8416-4769-b4d9-fdac891b417a"
      },
      "source": [
        "tokenizer.encode(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 14477,\n",
              " 21408,\n",
              " 16671,\n",
              " 3085,\n",
              " 2616,\n",
              " 2024,\n",
              " 8971,\n",
              " 2007,\n",
              " 24880,\n",
              " 3940,\n",
              " 17181,\n",
              " 1012,\n",
              " 102]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88gYiioDtFLW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d62844e-8394-4fa3-e101-84a86f206d07"
      },
      "source": [
        "\" \".join(['[CLS]', 'boston', 'is', 'a', 'city', 'in', 'massachusetts', '.', 'it', 'is', 'not', 'only', 'the', 'largest', 'city', 'in', 'massachusetts', ',', 'but', 'the', 'capital', '.', '[SEP]'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] boston is a city in massachusetts . it is not only the largest city in massachusetts , but the capital . [SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9XV5ONVqRjQ"
      },
      "source": [
        "In the case of encoding multiple sequences (say for natural language inference tasks), separate those sentences with special delimiters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvYbxCw4qSNn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "2ad4a22d-23b3-4af4-c72e-7f688e15c30d"
      },
      "source": [
        "# Special tokens are included that delimit sequences\n",
        "sents = [\"Boston is a city in Massachusetts. It is not only the largest city in Massachusetts, but the capital.\",\n",
        "         \"Where is Boston?\"]\n",
        "tokens = ([tokenizer.cls_token] + tokenizer.tokenize(sents[0]) + \n",
        "          [tokenizer.sep_token] + tokenizer.tokenize(sents[1]) + \n",
        "          [tokenizer.sep_token])\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "list(zip(ids, tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(101, '[CLS]'),\n",
              " (3731, 'boston'),\n",
              " (2003, 'is'),\n",
              " (1037, 'a'),\n",
              " (2103, 'city'),\n",
              " (1999, 'in'),\n",
              " (4404, 'massachusetts'),\n",
              " (1012, '.'),\n",
              " (2009, 'it'),\n",
              " (2003, 'is'),\n",
              " (2025, 'not'),\n",
              " (2069, 'only'),\n",
              " (1996, 'the'),\n",
              " (2922, 'largest'),\n",
              " (2103, 'city'),\n",
              " (1999, 'in'),\n",
              " (4404, 'massachusetts'),\n",
              " (1010, ','),\n",
              " (2021, 'but'),\n",
              " (1996, 'the'),\n",
              " (3007, 'capital'),\n",
              " (1012, '.'),\n",
              " (102, '[SEP]'),\n",
              " (2073, 'where'),\n",
              " (2003, 'is'),\n",
              " (3731, 'boston'),\n",
              " (1029, '?'),\n",
              " (102, '[SEP]')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp5Fi8eEB3hV"
      },
      "source": [
        "This use case can also be simplified by using `tokenizer.encode_plus`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSq7YJHPB8-T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9cad4ca4-01c8-4e7b-d674-ccdb74476ed9"
      },
      "source": [
        "tokenizer.encode_plus(*sents, return_token_type_ids=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1],\n",
              " 'input_ids': [101,\n",
              "  3731,\n",
              "  2003,\n",
              "  1037,\n",
              "  2103,\n",
              "  1999,\n",
              "  4404,\n",
              "  1012,\n",
              "  2009,\n",
              "  2003,\n",
              "  2025,\n",
              "  2069,\n",
              "  1996,\n",
              "  2922,\n",
              "  2103,\n",
              "  1999,\n",
              "  4404,\n",
              "  1010,\n",
              "  2021,\n",
              "  1996,\n",
              "  3007,\n",
              "  1012,\n",
              "  102,\n",
              "  2073,\n",
              "  2003,\n",
              "  3731,\n",
              "  1029,\n",
              "  102],\n",
              " 'token_type_ids': [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNx2M298tv1h"
      },
      "source": [
        "To feed the text to the model, put the IDs for all sentences of one batch in a tensor, then call the model on it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScNCmhrWqqUz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bb23d18c-762e-4350-d10f-e0f6c814cff5"
      },
      "source": [
        "# Put the token IDs in a tensor of batch size 1\n",
        "tokens_tensor = torch.tensor([ids])\n",
        "tokens_tensor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 101, 3731, 2003, 1037, 2103, 1999, 4404, 1012, 2009, 2003, 2025, 2069,\n",
              "         1996, 2922, 2103, 1999, 4404, 1010, 2021, 1996, 3007, 1012,  102, 2073,\n",
              "         2003, 3731, 1029,  102]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ9SfyQ4qvze",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6f14572d-ddb3-44fb-b336-c1f609ca17f2"
      },
      "source": [
        "# Feed the batch to the model and get our hidden representations\n",
        "batch_hiddens, *_other = model(tokens_tensor)\n",
        "hiddens = batch_hiddens[0]\n",
        "hiddens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.3978,  0.2074, -0.5156,  ..., -0.5493,  0.5735,  0.5814],\n",
              "        [ 0.1739, -0.3144, -0.6636,  ..., -0.2763,  0.3885,  0.1835],\n",
              "        [-0.8951, -0.5945, -0.0142,  ..., -0.4121,  0.7583,  0.5488],\n",
              "        ...,\n",
              "        [ 0.3861, -0.4660, -0.6078,  ..., -0.3762, -0.6090, -0.0122],\n",
              "        [-0.5916, -1.0025, -1.0509,  ...,  0.3678, -0.0044,  0.1510],\n",
              "        [-1.5023, -0.2680, -0.8075,  ...,  0.0321,  0.3901,  0.1777]],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkI7E-9YUJEJ"
      },
      "source": [
        "**Context dependence**\n",
        "\n",
        "One of the key advantages of using transformer models like BERT is that the word embeddings they provide are context-dependent.\n",
        "\n",
        "Let's get some of these word embeddings from different sentences, and see how the representation of the word \"saw\" changes depending on the context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZA3O6UJ-5oT"
      },
      "source": [
        "# Each sentence is paired with the index that points to the location of the word we're interested in\n",
        "sequences = [(\"I saw the car\", 1),\n",
        "             (\"I cut the log with a saw\", 6),\n",
        "             (\"She saw the dog\", 1),\n",
        "             (\"I drilled a hole with a drill\", 6),\n",
        "             (\"Use a screwdriver to drive the screw\", 2)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caIIszTTwRMT"
      },
      "source": [
        "def get_embedding(sentence, loc):\n",
        "  seq_ids = tokenizer.encode(sentence)\n",
        "  seq_ids_tensor = torch.tensor([seq_ids])\n",
        "  hiddens, *_other = model(seq_ids_tensor)\n",
        "  return hiddens[0][loc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhbdjAhjCyEW"
      },
      "source": [
        "# Create a mapping from sentences to the word embeddings of the relevant words\n",
        "embeddings = {}\n",
        "for sentence, loc in sequences:\n",
        "  seq_ids = tokenizer.encode(sentence)\n",
        "  seq_ids_tensor = torch.tensor([seq_ids])\n",
        "  hiddens, *_other = model(seq_ids_tensor)\n",
        "  embeddings[sentence] = hiddens[0][loc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjkY0uV0VPt7"
      },
      "source": [
        "Compare \"saw\" in \"I saw the car\" with words in other sentences. Sentences with the *seeing* sense have high similarity.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o0E4XH_UxRy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9f6abcfa-c2be-4b28-c8ee-cf157eb772e8"
      },
      "source": [
        "cos_sim = torch.nn.functional.cosine_similarity\n",
        "ref, refloc = sequences[0]\n",
        "print(\"\\\"{}\\\" in \\\"{}\\\" has a cosine similarity of...\".format(tokenizer.tokenize(ref)[refloc], ref))\n",
        "for sent, loc in sequences:\n",
        "  print(\"{:3.2f} with \\\"{}\\\" in \\\"{}\\\"\".format(cos_sim(get_embedding(ref, refloc), get_embedding(sent, loc), dim=0),\n",
        "                                          tokenizer.tokenize(sent)[loc],\n",
        "                                          sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"saw\" in \"I saw the car\" has a cosine similarity of...\n",
            "1.00 with \"saw\" in \"I saw the car\"\n",
            "0.38 with \"saw\" in \"I cut the log with a saw\"\n",
            "0.64 with \"saw\" in \"She saw the dog\"\n",
            "0.39 with \"drill\" in \"I drilled a hole with a drill\"\n",
            "0.25 with \"screw\" in \"Use a screwdriver to drive the screw\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgWV8ev_Wuf5"
      },
      "source": [
        "Compare \"saw\" in \"I cut the log with the saw\" with other \"saw\"s and tools. The word is closer to other tools than it is other sense of the word \"saw\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUTadcrLW2uZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f557e961-2472-4d66-b559-808956c8f414"
      },
      "source": [
        "cos_sim = torch.nn.functional.cosine_similarity\n",
        "ref, refloc = sequences[1]\n",
        "print(\"\\\"{}\\\" in \\\"{}\\\" has a cosine similarity of...\".format(tokenizer.tokenize(ref)[refloc], ref))\n",
        "for sent, loc in sequences:\n",
        "  print(\"{:3.2f} with \\\"{}\\\" in \\\"{}\\\"\".format(cos_sim(get_embedding(ref, refloc), get_embedding(sent, loc), dim=0),\n",
        "                                          tokenizer.tokenize(sent)[loc],\n",
        "                                          sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"saw\" in \"I cut the log with a saw\" has a cosine similarity of...\n",
            "0.38 with \"saw\" in \"I saw the car\"\n",
            "1.00 with \"saw\" in \"I cut the log with a saw\"\n",
            "0.36 with \"saw\" in \"She saw the dog\"\n",
            "0.83 with \"drill\" in \"I drilled a hole with a drill\"\n",
            "0.64 with \"screw\" in \"Use a screwdriver to drive the screw\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDTvo9BmyZXy"
      },
      "source": [
        "Try experimenting with your own sentence and word choices!"
      ]
    }
  ]
}